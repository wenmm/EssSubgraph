{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import torch\n",
    "from torch_geometric.utils import remove_self_loops, to_undirected\n",
    "\n",
    "from typing import Optional, Callable, List\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.data import Data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from numpy import random\n",
    "import sys\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def generate_5CV_set(drivers,nondrivers,randseed):\n",
    "    \"\"\"\n",
    "    Generate 5CV splits.\n",
    "    :param drivers: List of canonical driver genes(positive samples)\n",
    "    :param nondrivers: List of nondriver genes(negative samples)\n",
    "    :param randseed: Random seed\n",
    "    :return: 5CV splits sorted in a dictionary\n",
    "    \"\"\"\n",
    "    # StratifiedKFold\n",
    "    X, y = drivers + nondrivers, np.hstack(([1]*len(drivers), [0]*len(nondrivers)))\n",
    "    skf = StratifiedKFold(n_splits=8,shuffle=True,random_state=randseed)\n",
    "    X_5CV = {}\n",
    "    cv_idx=1\n",
    "    for train, test in skf.split(X, y):\n",
    "        # train/test sorts the sample indices in X list.\n",
    "        # For each split, we should convert the indices in train/test to names\n",
    "        train_set=[]\n",
    "        test_set=[]\n",
    "        for i in train:\n",
    "            train_set.append(X[i])\n",
    "        for i in test:\n",
    "            test_set.append(X[i])\n",
    "\n",
    "\n",
    "        X_5CV['train_%d' % cv_idx] = torch.tensor(train_set)\n",
    "        X_5CV['test_%d' % cv_idx] = torch.tensor(test_set)\n",
    "        cv_idx += 1\n",
    "    return X_5CV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "delimiter = \" \"\n",
    "net_name = \"./data/string_net.txt\"\n",
    "#net_name = \"/home/hwen6/database/string_db/string_v9.1.txt\"\n",
    "#net_name = \"/home/hwen6/gongju/one_net_subnetwork/train_valid_G.csv\"\n",
    "#network_name = net_name.split(\"/\")[-1].split(\"_\")[0]\n",
    "network_name = \"string_version2\"\n",
    "\n",
    "# net_names = [\"./data/string_net.txt\",\n",
    "#              \"./data/BIOGRID_net.txt\",\n",
    "#              \"./data/CPDB_net.txt\",\n",
    "#              \"./data/HumanNet_net.txt\",\n",
    "#              \"./data/IREF_net.txt\",\n",
    "#              \"./data/PathwayCommons_net.txt\",\n",
    "#              \"./data/pcnet_net.txt\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graphs = [pd.read_csv(net_name, delimiter=delimiter, header=None)]\n",
    "for G in graphs:\n",
    "    if G.shape[1] < 3:\n",
    "        G[2] = pd.Series([1.0] * len(G))\n",
    "\n",
    "labels = []\n",
    "#label_names = [\"./data/mouse_gene_labels.json\"]\n",
    "label_names = [\"./data/dep_gene_labels.json\"]\n",
    "\n",
    "for label_name in label_names:\n",
    "    with open(label_name, \"r\") as f:\n",
    "        labels.append(json.load(f))\n",
    "\n",
    "\n",
    "node_sets = [np.union1d(G[0].values, G[1].values) for G in graphs]\n",
    "union = reduce(np.union1d, node_sets)\n",
    "\n",
    "#weights = torch.FloatTensor([1.0 for G in graphs])\n",
    "weights = torch.FloatTensor([1.0])\n",
    "\n",
    "masks = torch.FloatTensor([np.isin(union, nodes) for nodes in node_sets])\n",
    "masks = torch.t(masks)\n",
    "\n",
    "mapper = {name: idx for idx, name in enumerate(union)}\n",
    "\n",
    "e_lst = pd.read_table(filepath_or_buffer='./data/dep_common_essential_genes', sep='\\t', header=None, index_col=None,\n",
    "                  names=['essential'])\n",
    "\n",
    "#e_lst = pd.read_table(filepath_or_buffer='./data/Mouse_essential_genes', sep='\\t', header=None, index_col=None,\n",
    "#                  names=['essential'])\n",
    "\n",
    "e_lst = e_lst['essential'].values.tolist()\n",
    "\n",
    "# Nonessential genes (negative samples)\n",
    "ne_lst = pd.read_table(filepath_or_buffer='./data/dep_non_essential_genes', sep='\\t', header=None, index_col=None, names=['nonessential'])\n",
    "\n",
    "#ne_lst = pd.read_table(filepath_or_buffer='./data/Mouse_viable_genes', sep='\\t', header=None,\n",
    "#               index_col=None, names=['nonessential'])\n",
    "ne_lst = ne_lst['nonessential'].values.tolist()\n",
    "\n",
    "e_idx = [mapper[i] for i in e_lst if i in mapper and i in labels[0]]\n",
    "ne_idx = [mapper[i] for i in ne_lst if i in mapper  and i in labels[0]]\n",
    "\n",
    "\n",
    "n = 1\n",
    "k_sets_net = dict()\n",
    "for k in np.arange(0,10): # Randomly generate 5CV splits for ten times\n",
    "    k_sets_net[k] = []\n",
    "    randseed = (k+1)%100+(k+1)*5\n",
    "    cv = generate_5CV_set(e_idx,ne_idx,randseed)\n",
    "    for cv_idx in np.arange(1,6):\n",
    "        a = cv[\"train_%d\" % cv_idx]\n",
    "        b = cv[\"test_%d\" % cv_idx]\n",
    "        random.shuffle(a)\n",
    "        random.shuffle(b)\n",
    "        test_mask = b\n",
    "        train_mask = a[:int(len(cv[\"train_1\"])/10*9)]\n",
    "        valid_mask = a[int(len(cv[\"train_1\"])/10*9):]\n",
    "        k_sets_net[k].append((train_mask, valid_mask, test_mask))\n",
    "        n += 1\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "        #gene_feature = pd.read_csv('/home/hwen6/public_data/TCGA/XENA_pancancer/tcga_RSEM_gene_fpkm_gname_pc150.csv', sep=',',index_col=0)\n",
    "        #gene_feature = pd.read_csv('/home/hwen6/gongju/DeepHE/data/gene_dna_pep_network_feature.csv', sep=',',index_col=0)\n",
    "        #gene_feature = pd.read_csv('/home/hwen6/database/gene_expression_prediction/cancer_full_expression_pc50.csv', sep=',',index_col=0)\n",
    "#gene_feature = pd.read_csv('./data/cancer_full_expression.tsv', sep='\\t')\n",
    "#pca = PCA(n_components=50)\n",
    "\n",
    "#pca_data = gene_feature.drop(['sample'], axis=1)\n",
    "\n",
    "#pca_data = pd.DataFrame(pca_data,index=union).fillna(0)\n",
    "#reduced_matrix = pca.fit_transform(pca_data)\n",
    "gene_feature = pd.read_csv('/home/hwen6/database/gene_expression_prediction/cancer_full_expression_pc50.csv', sep=',',index_col=0)\n",
    "#reduced_matrix = pd.DataFrame(gene_feature)\n",
    "gene_df = pd.read_csv('/home/hwen6/database/gene_expression_prediction/cancer_full_expression.tsv', sep='\\t')\n",
    "#reduced_matrix.index = gene_feature['sample']\n",
    "\n",
    "pyg_graphs = []\n",
    "for i in range(10):\n",
    "    for cv_run in range(5):\n",
    "        train_mask, valid_mask, test_mask = k_sets_net[i][cv_run]\n",
    "        train_valid_idx_list = set(train_mask.tolist() + valid_mask.tolist())\n",
    "        train_valid_gnames = {gname for gname, idx in mapper.items() if idx in train_valid_idx_list}\n",
    "        train_valid_G = G[G[0].isin(train_valid_gnames) & G[1].isin(train_valid_gnames)]\n",
    "\n",
    "        \n",
    "        e_lst = pd.read_table(filepath_or_buffer='./data/dep_common_essential_genes', sep='\\t', header=None, index_col=None,\n",
    "                  names=['essential'])\n",
    "\n",
    "        #e_lst = pd.read_table(filepath_or_buffer='./data/Mouse_essential_genes', sep='\\t', header=None, index_col=None,\n",
    "        #                  names=['essential'])\n",
    "\n",
    "        e_lst = e_lst['essential'].values.tolist()\n",
    "\n",
    "        # Nonessential genes (negative samples)\n",
    "        ne_lst = pd.read_table(filepath_or_buffer='./data/dep_non_essential_genes', sep='\\t', header=None, index_col=None, names=['nonessential'])\n",
    "\n",
    "        #ne_lst = pd.read_table(filepath_or_buffer='./data/Mouse_viable_genes', sep='\\t', header=None,\n",
    "        #               index_col=None, names=['nonessential'])\n",
    "        ne_lst = ne_lst['nonessential'].values.tolist()\n",
    "\n",
    "        e_idx = [mapper[i] for i in e_lst if i in mapper and i in labels[0]]\n",
    "        ne_idx = [mapper[i] for i in ne_lst if i in mapper  and i in labels[0]]\n",
    "\n",
    "        train_valid_gene_df = gene_df[gene_df['sample'].isin(train_valid_gnames)]\n",
    "\n",
    "        \n",
    "        pca = PCA(n_components=50)\n",
    "\n",
    "        train_valid_pca_data = train_valid_gene_df.drop(['sample'], axis=1)\n",
    "\n",
    "        all_data_f = gene_df[gene_df['sample'].isin(union)]\n",
    "\n",
    "        all_data_f = pd.DataFrame(all_data_f,index=union).fillna(0)\n",
    "        all_data = all_data_f.drop(['sample'], axis=1)\n",
    "\n",
    "\n",
    "        #scaler = StandardScaler().fit(train_valid_pca_data)\n",
    "        #train_scaled = scaler.transform(train_valid_pca_data)\n",
    "\n",
    "        #all_scaled = scaler.transform(all_data)\n",
    "\n",
    "        reduced_matrix = pca.fit(train_valid_pca_data)\n",
    "\n",
    "        \n",
    "\n",
    "        all_pca = pca.transform(all_data)\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "        gene_feature_index = pd.DataFrame(all_pca).reindex(union, fill_value=0)\n",
    "        feat_raw = scaler.fit_transform(np.abs(gene_feature_index))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #gene_feature = pd.read_csv('/home/hwen6/gongju/DeepHE/data/gene_dna_pep_feature_gname_uniq.csv', sep=',',index_col=0)\n",
    "        gene_feature_index = pd.DataFrame(gene_feature,index=union).fillna(0)\n",
    "\n",
    "        from sklearn import preprocessing\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        feat_raw = scaler.fit_transform(np.abs(gene_feature_index))\n",
    "\n",
    "        final_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02116056, 0.08041215, 0.27146386, ..., 0.10360078, 0.08012268,\n",
       "        0.1148902 ],\n",
       "       [0.45162576, 0.12093439, 0.73567221, ..., 0.04582405, 0.08049637,\n",
       "        0.14412393],\n",
       "       [0.6770235 , 0.14258315, 0.20089181, ..., 0.06759901, 0.02039906,\n",
       "        0.04266136],\n",
       "       ...,\n",
       "       [0.33420198, 0.08189742, 0.02921014, ..., 0.02446723, 0.02263919,\n",
       "        0.02478441],\n",
       "       [0.55236118, 0.04355629, 0.01502991, ..., 0.04594499, 0.09041962,\n",
       "        0.04536995],\n",
       "       [0.30603614, 0.00759395, 0.0081904 , ..., 0.02769054, 0.02851045,\n",
       "        0.01100272]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.utils import prepare_folder\n",
    "from utils.evaluator import Evaluator\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from models import SAGE_NeighSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.utils import to_undirected\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.utils import to_undirected\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "eval_metric = 'auc'\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj( name ):\n",
    "    \"\"\"\n",
    "    Load dataset from pickle file.\n",
    "    :param name: Full pathname of the pickle file\n",
    "    :return: Dataset type of dictionary\n",
    "    \"\"\"\n",
    "    with open( name , 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_obj(\"test_cancer_full_expression_pc50_PCA_exclude_test_string_version2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13137"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9223372036854775808],\n",
       "        [-9223372036854775808],\n",
       "        [                   0],\n",
       "        ...,\n",
       "        [                   0],\n",
       "        [                   0],\n",
       "        [                   1]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.adj_t = data.adj_t.to_symmetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = {'train':data.train_mask, 'valid':data.valid_mask, 'test':data.test_mask}\n",
    "train_idx = split_idx['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8145"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   11,    11,    11,  ..., 10706,  9656,  5610])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1293"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  49,   49,  116,  ..., 8778, 6702, 4939])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13137"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13137"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[data.train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "my_list = list(data.y[data.valid_mask])\n",
    "unique_list = []\n",
    "[unique_list.append(x) for x in my_list if x not in unique_list]\n",
    "print(unique_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13137"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.x\n",
    "x = (x-x.mean(0))/x.std(0)\n",
    "data.x = x\n",
    "if data.y.dim()==2:\n",
    "    data.y = data.y.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import prepare_folder\n",
    "from utils.evaluator import Evaluator\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from models import SAGE_NeighSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.utils import to_undirected\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.utils import to_undirected\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "eval_metric = 'auc'\n",
    "import time\n",
    "\n",
    "\n",
    "sage_neighsampler_parameters = {'lr':0.003\n",
    "              , 'num_layers':2\n",
    "              , 'hidden_channels':128\n",
    "              , 'dropout':0.0\n",
    "              , 'batchnorm': False\n",
    "              , 'l2':5e-7\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch, train_loader, model, data, train_idx, optimizer, device, no_conv=False):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=train_idx.size(0), ncols=80)\n",
    "    pbar.set_description(f'Epoch {epoch:02d}')\n",
    "\n",
    "    total_loss = total_correct = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x[n_id], adjs)\n",
    "        loss = F.nll_loss(out, data.y[n_id[:batch_size]])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        pbar.update(batch_size)\n",
    "\n",
    "    pbar.close()\n",
    "    loss = total_loss / len(train_loader)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(layer_loader, model, data, split_idx, device, no_conv=False):\n",
    "    # data.y is labels of shape (N, ) \n",
    "    model.eval()\n",
    "    \n",
    "    out = model.inference(data.x, layer_loader, device)\n",
    "#     out = model.inference_all(data)\n",
    "    y_pred = out.exp()  # (N,num_classes)   \n",
    "    \n",
    "    losses = dict()\n",
    "    for key in ['train', 'valid', 'test']:\n",
    "        node_id = split_idx[key]\n",
    "        node_id = node_id.to(device)\n",
    "        losses[key] = F.nll_loss(out[node_id], data.y[node_id]).item()\n",
    "            \n",
    "    return losses, y_pred\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference_test(layer_loader, model, data, device, no_conv=False):\n",
    "    # data.y is labels of shape (N, ) \n",
    "    model.eval()\n",
    "    \n",
    "    out = model.inference(data.x, layer_loader, device)\n",
    "#     out = model.inference_all(data)\n",
    "    y_pred = out.exp()  # (N,num_classes)   \n",
    "                \n",
    "    return y_pred\n",
    "\n",
    "def load_obj( name ):\n",
    "    \"\"\"\n",
    "    Load dataset from pickle file.\n",
    "    :param name: Full pathname of the pickle file\n",
    "    :return: Dataset type of dictionary\n",
    "    \"\"\"\n",
    "    with open( name , 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_obj(\"test_cancer_full_expression_pc50_PCA_exclude_test_string_version2.pkl\")\n",
    "\n",
    "device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlabels = 2\n",
    "    #networks_name = ['BioGrid','CPDB','pcnet','pcnet','pcnet','pcnet','string']\n",
    "  \n",
    "for n in range(1):\n",
    "    data = dataset[n]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "\n",
    "\n",
    "    model_dir = prepare_folder(\"dep_pc50_version2_{}_{}\".format('string', n), 'sage_neighsampler')\n",
    "    split_idx = {'train':data.train_mask, 'valid':data.valid_mask, 'test':data.test_mask}\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "    data = data.to(device)\n",
    "        \n",
    "\n",
    "    x = data.x\n",
    "    x = (x-x.mean(0))/x.std(0)\n",
    "    data.x = x\n",
    "    if data.y.dim()==2:\n",
    "        data.y = data.y.squeeze(1)\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    train_loader = NeighborSampler(data.adj_t, node_idx=train_idx, sizes=[25, 10], batch_size=1024, shuffle=True, num_workers=12)\n",
    "\n",
    "    layer_loader = NeighborSampler(data.adj_t, node_idx=None, sizes=[-1], batch_size=4096, shuffle=False, num_workers=12)    \n",
    "\n",
    "    para_dict = sage_neighsampler_parameters\n",
    "    model_para = sage_neighsampler_parameters.copy()\n",
    "    model_para.pop('lr')\n",
    "    model_para.pop('l2')\n",
    "    model = SAGE_NeighSampler(in_channels = data.x.size(-1), out_channels = nlabels, **model_para).to(device)\n",
    "\n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=para_dict['lr'], weight_decay=para_dict['l2'])\n",
    "    min_valid_loss = 1e8\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeighborSampler(sizes=[-1])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_subnet",
   "language": "python",
   "name": "py38_subnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
